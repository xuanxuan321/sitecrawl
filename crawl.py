# pip install -U crawl4ai
import asyncio, time, xml.etree.ElementTree as ET
from crawl4ai import AsyncUrlSeeder, SeedingConfig, AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy

DOMAIN = "ant-design.antgroup.com"   # åªå†™åŸŸåï¼Œå¦‚ react.dev

async def discover_urls(domain: str) -> list[str]:
    # 1) å…ˆç”¨ URL æ’­ç§ä» sitemap è·å–ï¼ˆæ”¯æŒ sitemap ç´¢å¼•ï¼‰
    try:
        async with AsyncUrlSeeder() as seeder:
            cfg = SeedingConfig(source="sitemap", extract_head=False, live_check=True)
            rows = await seeder.urls(domain, cfg)  # rows: [{"url": "...", "status": "valid", ...}, ...]
            urls = [r["url"] for r in rows if r.get("status") in (None, "valid")]
            if urls:
                return sorted(set(urls))
    except Exception:
        pass

    # 2) å›é€€ï¼šç”¨ BFS åœ¨ç«™å†…æœ€å¤šçˆ¬ 2 å±‚æŠŠ URL å‘ç°å‡ºæ¥
    async with AsyncWebCrawler() as crawler:
        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=5, include_external=False, max_pages=1000),
            only_text=True,
        )
        results = await crawler.arun(f"https://{domain}", config=config)
        return sorted({r.url for r in results if getattr(r, "success", False)})

def write_sitemap(urls: list[str], out_path="sitemap.xml"):
    urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
    today = time.strftime("%Y-%m-%d")
    for u in urls:
        url = ET.SubElement(urlset, "url")
        ET.SubElement(url, "loc").text = u
        ET.SubElement(url, "lastmod").text = today
    ET.ElementTree(urlset).write(out_path, encoding="utf-8", xml_declaration=True)

def analyze_sitemap(urls: list[str], domain: str):
    """åˆ†æç½‘ç«™åœ°å›¾å¹¶è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š ç½‘ç«™åˆ†ææŠ¥å‘Š - {domain}")
    print("=" * 50)
    print(f"æ€»å…±å‘ç° {len(urls)} ä¸ªé¡µé¢")
    
    # æŒ‰è·¯å¾„åˆ†ç±»ç»Ÿè®¡
    categories = {}
    for url in urls:
        # ç§»é™¤åŸŸåéƒ¨åˆ†ï¼Œè·å–è·¯å¾„
        path_part = url.replace(f"https://{domain}", "").replace(f"http://{domain}", "")
        if not path_part or path_part == "/" or path_part == "":
            category = "home"
        else:
            # è·å–ç¬¬ä¸€çº§è·¯å¾„ä½œä¸ºåˆ†ç±»
            path_parts = path_part.strip("/").split("/")
            category = path_parts[0] if path_parts[0] else "home"
        
        categories[category] = categories.get(category, 0) + 1
    
    print(f"\nğŸ“‚ ä¸»è¦é¡µé¢åˆ†ç±»:")
    for category, count in sorted(categories.items()):
        print(f"  {category}: {count} é¡µé¢")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹URL
    print(f"\nğŸ”— ç¤ºä¾‹é¡µé¢:")
    for i, url in enumerate(urls[:5]):
        print(f"  {i+1}. {url}")
    
    if len(urls) > 5:
        print(f"  ... è¿˜æœ‰ {len(urls) - 5} ä¸ªé¡µé¢")
    
    print(f"\nâœ… ç½‘ç«™åœ°å›¾å·²ä¿å­˜åˆ°: sitemap.xml")

async def main():
    urls = await discover_urls(DOMAIN)
    write_sitemap(urls)
    print(f"Got {len(urls)} urls â†’ sitemap.xml")
    
    # åˆ†æå¹¶è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    analyze_sitemap(urls, DOMAIN)

if __name__ == "__main__":
    asyncio.run(main())